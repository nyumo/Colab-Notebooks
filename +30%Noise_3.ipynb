{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit    # フィッティング用\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import time\n",
    "import csv\n",
    "#訓練データの作成を早くすることを目的とした．\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE' #意味はわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:20.118541955947876[sec]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_num = 10000 #訓練データの数\n",
    "test_num = 1000 #テストデータの数\n",
    "data_size = 100 #配列の大きさ\n",
    "\n",
    "x_train = np.zeros((train_num,data_size))\n",
    "y_train = np.zeros((train_num,data_size))\n",
    "tx_train = np.zeros((train_num, 3))\n",
    "ty_train = np.zeros((train_num, 3))\n",
    "t_1_train = np.zeros(train_num)\n",
    "x_test = np.zeros((test_num,data_size))\n",
    "y_test = np.zeros((test_num,data_size))\n",
    "tx_test = np.zeros((test_num, 3))\n",
    "ty_test = np.zeros((test_num, 3))\n",
    "t_1_test = np.zeros(test_num)\n",
    "\n",
    "# ガウシアンビームの関数の定義\n",
    "def gaussian_beam(x,a,b,c,d):\n",
    "    return  a * np.exp(-2*(x-b)*(x-b)/c/c) + d\n",
    "\n",
    "# ガウシアンビームのパラメータ\n",
    "i0 = 1.0\n",
    "x0 = np.arange(0.0, 2.0, 0.1)\n",
    "y0 = np.arange(0.0, 2.0, 0.1)\n",
    "w0 = 5.0\n",
    "h0 = 0.0\n",
    "\n",
    "# 信号に対するノイズ量[%]\n",
    "NOISE = 30\n",
    "\n",
    "\n",
    "def create_data(N, n_N, x0_N, y0_N, w0_x_N, w0_y_N, h0_x_N, h0_y_N,i0, x0, y0, w0, h0, x, y, tx, ty, t1):\n",
    "    n_N      = np.arange(N) +1\n",
    "    x0_N     = np.zeros(N)   # N回分のx0,w0を格納する配列\n",
    "    y0_N     = np.zeros(N)\n",
    "    w0_x_N   = np.zeros(N)\n",
    "    w0_y_N   = np.zeros(N)\n",
    "    h0_x_N   = np.zeros(N)\n",
    "    h0_y_N   = np.zeros(N)\n",
    "    \n",
    "    x0 = x0.repeat(N/20) #20stepの中心値\n",
    "    y0 = y0.repeat(N/20)\n",
    "    \n",
    "    # x配列とy配列\n",
    "    x_array = np.arange(-50, 50, 1.0)                         # x配列\n",
    "    y_array = np.arange(-50, 50, 1.0)                         # y配列\n",
    "    nx = len(x_array)\n",
    "    ny = len(y_array)\n",
    "    intensity = np.zeros((N, nx, ny))                            # ノイズを含まない2次元強度分布\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            intensity[:,i,j] = i0 * np.exp(-2*((x_array[i]-x0[i])*(x_array[i]-x0[i]) + (y_array[j]-y0[j])*(y_array[j]-y0[j]))/w0/w0)\n",
    "            \n",
    "    # 2次元の強度分布にノイズを付与\n",
    "    intensity_noise = np.zeros((N, nx, ny))\n",
    "    noise = (np.random.rand(N*nx*ny)-0.5)*i0*NOISE*0.01   # プラスマイナスNOISE%のノイズ(一様分布), (np.random.rand(nx*ny)-0.5)*2の部分が-1から1までの乱数になる\n",
    "    noise = noise.reshape((N,nx,ny))\n",
    "    intensity_noise = intensity + noise\n",
    "    \n",
    "    # 最大強度を取る位置における強度プロファイル\n",
    "    profile_x = np.zeros(nx)\n",
    "    profile_y = np.zeros(ny)\n",
    "    idx = intensity_noise.max(axis=1).argmax(axis=1)\n",
    "    idy = intensity_noise.max(axis=2).argmax(axis=1)\n",
    "    \n",
    "    for n in range(N):\n",
    "        x[n] = intensity_noise[n,:,idy[n]]\n",
    "        y[n] = intensity_noise[n,idx[n],:]\n",
    "        tx[n] = (i0, x0[n], w0)\n",
    "        ty[n] = (i0, y0[n], w0)\n",
    "        t1[n] = x0[n]\n",
    "        \n",
    "        param_ini_x = np.array([i0, x0[n], w0, h0])  # フィッティングの初期値 N×4 (ここではデータから推定は行わない)\n",
    "        param_ini_y = np.array([i0, y0[n], w0, h0])\n",
    "        # x方向のプロファイルの非線形フィッティング\n",
    "        param, cov  = curve_fit(gaussian_beam, x_array, profile_x, p0=param_ini_x, maxfev=2000)\n",
    "        i0_x        = param[0]\n",
    "        x0_N[n]     = param[1]\n",
    "        w0_x_N[n]   = param[2]\n",
    "        h0_x_N[n]   = param[3]\n",
    "        # y方向のプロファイルの非線形フィッティング\n",
    "        param, cov  = curve_fit(gaussian_beam, y_array, profile_y, p0=param_ini_y, maxfev=2000)\n",
    "        i0_y        = param[0]\n",
    "        y0_N[n]     = param[1]\n",
    "        w0_y_N[n]   = param[2]\n",
    "        h0_y_N[n]   = param[3]\n",
    "\n",
    "    return N, n_N, x0_N, y0_N, w0_x_N, w0_y_N, h0_x_N, h0_y_N, x, y, tx, ty\n",
    "\n",
    "# N回繰り返しフィッティングを行う\n",
    "N        = 10000\n",
    "n_N      = np.zeros(N)\n",
    "x0_N     = np.zeros(N)   # N回分のx0,w0を格納する配列\n",
    "y0_N     = np.zeros(N)\n",
    "w0_x_N   = np.zeros(N)\n",
    "w0_y_N   = np.zeros(N)\n",
    "h0_x_N   = np.zeros(N)\n",
    "h0_y_N   = np.zeros(N)\n",
    "\n",
    "create_data(train_num, n_N, x0_N, y0_N, w0_x_N, w0_y_N, h0_x_N, h0_y_N, \n",
    "            i0, x0, y0, w0, h0, x_train, y_train, tx_train, ty_train, t_1_train)\n",
    "\n",
    "# N回繰り返しフィッティングを行う\n",
    "N        = 1000\n",
    "n_N      = np.zeros(N)\n",
    "x0_N     = np.zeros(N)   # N回分のx0,w0を格納する配列\n",
    "y0_N     = np.zeros(N)\n",
    "w0_x_N   = np.zeros(N)\n",
    "w0_y_N   = np.zeros(N)\n",
    "h0_x_N   = np.zeros(N)\n",
    "h0_y_N   = np.zeros(N)\n",
    "\n",
    "create_data(test_num, n_N, x0_N, y0_N, w0_x_N, w0_y_N, h0_x_N, h0_y_N, \n",
    "            i0, x0, y0, w0, h0, x_test, y_test, tx_test, ty_test, t_1_test)\n",
    "\n",
    "t_train = t_1_train\n",
    "t_test = t_1_test\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データ,テストデータの保存\n",
    "save_x_train = \"/Users/nagaiyuma/Desktop/maindata/x_train.csv\"\n",
    "save_t_train = \"/Users/nagaiyuma/Desktop/maindata/t_train.csv\"\n",
    "save_x_test = \"/Users/nagaiyuma/Desktop/maindata/x_test.csv\"\n",
    "save_t_test = \"/Users/nagaiyuma/Desktop/maindata/t_test.csv\"\n",
    "\n",
    "with open(save_x_train,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(x_train)\n",
    "with open(save_t_train,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(t_train)\n",
    "with open(save_x_test,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(x_test)\n",
    "with open(save_t_test,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 100, 50)           200       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 50, 50)            7550      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 10, 10)            1510      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 20,403\n",
      "Trainable params: 20,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 5.5750 - mae: 1.9325 - val_loss: 3.3781 - val_mae: 1.6804\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 3.3043 - mae: 1.5063 - val_loss: 1.2543 - val_mae: 1.0336\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 2.3219 - mae: 1.1804 - val_loss: 0.7462 - val_mae: 0.8148\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 1.2970 - mae: 0.8745 - val_loss: 0.2778 - val_mae: 0.4766\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 1.0691 - mae: 0.7602 - val_loss: 0.2378 - val_mae: 0.4310\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 1.0326 - mae: 0.7386 - val_loss: 0.2548 - val_mae: 0.4588\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.9986 - mae: 0.7212 - val_loss: 0.1776 - val_mae: 0.3727\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.9723 - mae: 0.7065 - val_loss: 0.1869 - val_mae: 0.3848\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.9289 - mae: 0.6868 - val_loss: 0.2123 - val_mae: 0.4121\n",
      "Epoch 10/50\n",
      "  600/10000 [>.............................] - ETA: 9s - loss: 0.8584 - mae: 0.6570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6761da5ce60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m  validation_data=(test_x, test_t))\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CNNの実装\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "batch_size = 200  # 訓練データを200ずつのデータに分けて学習させる\n",
    "epochs = 50 # 訓練データを繰り返し学習させる数\n",
    "\n",
    "#データ形式の変更(シーケンス長, パラメータ数)\n",
    "train_x = x_train.reshape(-1, 100, 1)\n",
    "train_t = tx_train\n",
    "test_x = x_test.reshape(-1, 100, 1)\n",
    "test_t = tx_test\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(50, 3, padding='same', input_shape=(100, 1), activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "model.add(MaxPooling1D(2, padding='same'))\n",
    "model.add(Conv1D(50, 3, padding='same', activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "model.add(MaxPooling1D(5, padding='same'))\n",
    "model.add(Conv1D(10, 3, padding='same', activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='relu', kernel_initializer=\"he_nomal\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse',\n",
    " optimizer='adam',\n",
    " metrics=['mae'])\n",
    "\n",
    "history = model.fit(train_x, train_t,\n",
    " batch_size=batch_size,\n",
    " epochs=epochs,\n",
    " verbose=1,\n",
    " validation_data=(test_x, test_t))\n",
    "\n",
    "score = model.evaluate(test_x, test_t, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelの保存\n",
    "save_model_path = \"/Users/nagaiyuma/Desktop/maindata/model_cnn.h5\"\n",
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0952404 1.0981549 1.0905627 1.0918893 1.0983261 1.0943263 1.0901543\n",
      " 1.0934758 1.092184  1.092725  1.1009378 1.0890833 1.0947455 1.0914627\n",
      " 1.0833505 1.102924  1.1064401 1.0930467 1.0901797 1.0917666 1.0921512\n",
      " 1.0966629 1.090691  1.0928748 1.0952435 1.0939738 1.0880044 1.0940893\n",
      " 1.0894383 1.1034738 1.0946383 1.0928923 1.0917332 1.0930401 1.0990136\n",
      " 1.0974483 1.0934081 1.0942196 1.0808322 1.0893888 1.0902541 1.088207\n",
      " 1.1039424 1.0944775 1.0921052 1.0887935 1.0913438 1.0914543 1.1013227\n",
      " 1.0938206 1.0991504 1.0935855 1.0953867 1.0907693 1.092255  1.088342\n",
      " 1.0926437 1.0996927 1.0862145 1.0919836 1.0974971 1.1003898 1.0949007\n",
      " 1.0846772 1.0902739 1.1040245 1.0885185 1.0894084 1.0897561 1.087484\n",
      " 1.1016241 1.1029551 1.0842553 1.0863824 1.0899699 1.0940529 1.1016811\n",
      " 1.093326  1.0973867 1.0827389 1.093554  1.0973713 1.1003907 1.0792155\n",
      " 1.0892644 1.0913479 1.0881565 1.0887511 1.097034  1.0892444 1.0902773\n",
      " 1.0967995 1.0909938 1.0894047 1.0897228 1.0958344 1.0998472 1.0977397\n",
      " 1.0954643 1.0900942 1.0987489 1.0912125 1.0856589 1.0910559 1.1015571\n",
      " 1.0914513 1.0943508 1.0845369 1.0908276 1.0896243 1.0911343 1.0872838\n",
      " 1.0906646 1.0762744 1.0876558 1.0943165 1.0924844 1.097714  1.0971112\n",
      " 1.094929  1.0976391 1.0872822 1.0979686 1.0882404 1.0918553 1.1047492\n",
      " 1.0968753 1.0968341 1.086273  1.0969813 1.0968761 1.0902696 1.0802882\n",
      " 1.092055  1.0923507 1.0972247 1.0998819 1.0933336 1.0918083 1.0933954\n",
      " 1.0819492 1.0993114 1.0950524 1.0984046 1.0878781 1.0876766 1.091112\n",
      " 1.0969089 1.086467  1.0829303 1.0850035 1.0842922 1.1025903 1.0897226\n",
      " 1.0935674 1.0951703 1.084983  1.0947759 1.0874938 1.0942032 1.0969666\n",
      " 1.0952258 1.0966195 1.0880405 1.0961661 1.0980152 1.095887  1.0863577\n",
      " 1.0939865 1.0916939 1.0900172 1.0954875 1.086558  1.092658  1.1009128\n",
      " 1.0933707 1.092128  1.0886738 1.090394  1.0963408 1.0922279 1.092227\n",
      " 1.0878314 1.0958437 1.0845622 1.0971085 1.0893205 1.0926789 1.0934551\n",
      " 1.0951357 1.0942931 1.0901364 1.0961679 1.0961361 1.0976624 1.0970902\n",
      " 1.0920774 1.0925916 1.0880694 1.0866429 1.0941198 1.0969169 1.0856973\n",
      " 1.0928671 1.172375  1.0989231 1.0947919 1.0948297 1.0912951 1.0904763\n",
      " 1.0910796 1.0916411 1.0897838 1.0902362 1.1030208 1.0908793 1.0951747\n",
      " 1.090695  1.0952501 1.090322  1.0844731 1.083764  1.1022115 1.097078\n",
      " 1.0960555 1.0962687 1.0861857 1.0824245 1.0844785 1.0823135 1.0979143\n",
      " 1.0934794 1.0905948 1.0907612 1.098386  1.0879508 1.11274   1.0839045\n",
      " 1.0766855 1.087667  1.0919732 1.0907555 1.0923084 1.089452  1.0940285\n",
      " 1.110995  1.0947272 1.0900766 1.0921139 1.0976902 1.094477  1.0816126\n",
      " 1.0883772 1.086698  1.0881674 1.0932927 1.0917847 1.0959313 1.0877222\n",
      " 1.0933411 1.0920508 1.0941355 1.093093  1.0928023 1.0962908 1.0935185\n",
      " 1.0963848 1.0955267 1.0924743 1.0915163 1.0968167 1.1021385 1.103236\n",
      " 1.0893596 1.0961624 1.0841172 1.094798  1.0921978 1.0927099 1.095536\n",
      " 1.0916622 1.0950875 1.0848002 1.0869805 1.0953147 1.0997769 1.0882968\n",
      " 1.1008071 1.0935235 1.0927126 1.0977776 1.0999631 1.089822  1.0873752\n",
      " 1.0898628 1.0887765 1.0853622 1.08469   1.0874517 1.0952797 1.0952759\n",
      " 1.0899142 1.0942894 1.0914153 1.087775  1.0908666 1.0933756 1.0902154\n",
      " 1.1004002 1.0884144 1.0797033 1.0867558 1.0880343 1.0858682 1.0934894\n",
      " 1.0958881 1.0906879 1.0969106 1.0938015 1.089155  1.0936847 1.0945055\n",
      " 1.0955396 1.0997102 1.1008695 1.0926397 1.099094  1.0896589 1.0858729\n",
      " 1.0919454 1.0927271 1.0941713 1.0968007 1.0935148 1.0915406 1.0877271\n",
      " 1.0933142 1.085213  1.0942818 1.0920773 1.0881746 1.0927972 1.0890164\n",
      " 1.086371  1.083237  1.0868187 1.0983768 1.0911868 1.0944381 1.0887306\n",
      " 1.0866911 1.1044152 1.0869707 1.0940994 1.0888962 1.0933313 1.0906779\n",
      " 1.1013894 1.0908774 1.0945789 1.1032392 1.0951257 1.0986714 1.0887806\n",
      " 1.0951699 1.0963894 1.0979685 1.0918539 1.0900356 1.093379  1.0938244\n",
      " 1.0953256 1.1052275 1.0901288 1.0906225 1.0901248 1.095291  1.096148\n",
      " 1.085156  1.0939602 1.0943328 1.0946236 1.1000512 1.0959883 1.0848525\n",
      " 1.095293  1.0904678 1.0907102 1.0948597 1.0907427 1.0818559 1.096128\n",
      " 1.0911665 1.0951328 1.0867932 1.0901537 1.0926989 1.1012836 1.0926456\n",
      " 1.0851908 1.0898702 1.0955393 1.0909008 1.0915872 1.0998173 1.0905348\n",
      " 1.0923836 1.0922399 1.0948169 1.0876727 1.0992049 1.0928934 1.0930042\n",
      " 1.0987948 1.0939603 1.0944592 1.090256  1.0937699 1.0910367 1.096296\n",
      " 1.0959    1.0958937 1.0836971 1.0820655 1.0913942 1.0942593 1.0919094\n",
      " 1.0884709 1.0873923 1.0831516 1.0925795 1.0922397 1.0861675 1.0989714\n",
      " 1.0909035 1.0932794 1.0843756 1.0933716 1.0918086 1.0843657 1.0892835\n",
      " 1.0985914 1.0909534 1.1013899 1.0967803 1.0871022 1.0962468 1.092514\n",
      " 1.0908698 1.0914989 1.0915902 1.09255   1.09159   1.0947142 1.1058384\n",
      " 1.0877825 1.0904787 1.0948738 1.0838642 1.1042161 1.0935078 1.0936489\n",
      " 1.1018617 1.0965624 1.0889977 1.094942  1.0961643 1.08411   1.0957847\n",
      " 1.0872736 1.0877239 1.096172  1.0934819 1.1013385 1.0946548 1.0892129\n",
      " 1.0932374 1.0967758 1.0908155 1.0950636 1.092701  1.0822161 1.0969527\n",
      " 1.0904013 1.0973141 1.0954523 1.0917606 1.0960833 1.0858332 1.0917519\n",
      " 1.0994613 1.089801  1.0946083 1.0900623 1.0888951 1.0911474 1.0927639\n",
      " 1.0867339 1.0963798 1.0880971 1.0929686 1.0925841 1.0938809 1.0939639\n",
      " 1.0890927 1.0936365 1.1009696 1.096544  1.0846999 1.0929053 1.0923448\n",
      " 1.0931578 1.0869763 1.0908643 1.0878268 1.0868659 1.0936902 1.0953993\n",
      " 1.0888791 1.0921608 1.0918193 1.0925537 1.0899168 1.0843072 1.0881139\n",
      " 1.0892961 1.0895004 1.0929319 1.0864259 1.0940895 1.0979314 1.1000547\n",
      " 1.1215254 1.0915713 1.0874311 1.089514  1.095743  1.0980805 1.0866709\n",
      " 1.0998094 1.0979294 1.0923586 1.0938756 1.0858701 1.0879848 1.095803\n",
      " 1.0898633 1.0868309 1.091339  1.0952687 1.0920637 1.0923238 1.095714\n",
      " 1.0865358 1.0963963 1.0992846 1.0860064 1.0985289 1.0965548 1.0919099\n",
      " 1.090082  1.0977179 1.0889105 1.0944147 1.0908555 1.0878274 1.091233\n",
      " 1.0922508 1.0973353 1.0956714 1.0927675 1.0931005 1.0902569 1.0878392\n",
      " 1.0910413 1.0965022 1.094876  1.0888672 1.0908058 1.0903697 1.0960872\n",
      " 1.0944501 1.0892937 1.0788288 1.0921665 1.0987126 1.0921074 1.0920193\n",
      " 1.0902613 1.1001148 1.0853292 1.0870962 1.0895278 1.0969733 1.0939596\n",
      " 1.0886643 1.0948155 1.0902854 1.0960165 1.0872548 1.0932896 1.0939256\n",
      " 1.0984384 1.0878617 1.0919218 1.1011399 1.0913104 1.0870732 1.079721\n",
      " 1.0903304 1.0880028 1.089073  1.0941728 1.0846375 1.0847551 1.0970526\n",
      " 1.0942335 1.0910201 1.089505  1.0910234 1.0905068 1.0878636 1.093488\n",
      " 1.0982403 1.0960566 1.0845228 1.0901581 1.1184746 1.0967475 1.0925674\n",
      " 1.0989656 1.0965086 1.0933369 1.0935793 1.0993257 1.0956142 1.0887445\n",
      " 1.1016065 1.0900815 1.101406  1.0892235 1.1019604 1.0896727 1.0957408\n",
      " 1.0977086 1.0933037 1.0846806 1.0945358 1.0919287 1.0927007 1.0880644\n",
      " 1.0997392 1.0820603 1.0910455 1.0952859 1.0971965 1.0958112 1.0952107\n",
      " 1.0950209 1.0935795 1.0866157 1.0967098 1.0922998 1.0996556 1.0897192\n",
      " 1.0902526 1.0918632 1.0905815 1.0919755 1.0930555 1.0999523 1.0999906\n",
      " 1.0934601 1.0878392 1.0983536 1.091014  1.0930684 1.0941944 1.0967736\n",
      " 1.0861105 1.0911473 1.0956265 1.0912528 1.1029072 1.0846776 1.0954869\n",
      " 1.088762  1.086471  1.0833597 1.086154  1.0965589 1.0865881 1.0983773\n",
      " 1.0923842 1.0975913 1.0925659 1.1045686 1.1225523 1.1025003 1.0877506\n",
      " 1.0892103 1.0896875 1.0920339 1.0934705 1.0992981 1.0903039 1.085185\n",
      " 1.0911667 1.0863813 1.1018755 1.0971951 1.1093506 1.0919907 1.0860066\n",
      " 1.0956212 1.0932052 1.0881997 1.0960805 1.0843289 1.0942954 1.0948136\n",
      " 1.0978842 1.0900838 1.0882491 1.0963044 1.1016619 1.0924133 1.0936067\n",
      " 1.0951889 1.0898662 1.0897644 1.097397  1.0909835 1.0942283 1.0963154\n",
      " 1.0930796 1.0940956 1.0941477 1.094861  1.0926543 1.085927  1.0868969\n",
      " 1.0952308 1.0902456 1.0996559 1.0953358 1.0841616 1.0891756 1.1013708\n",
      " 1.0919802 1.0887072 1.0989327 1.0975416 1.0978612 1.0993526 1.1028119\n",
      " 1.0988095 1.0780141 1.0953052 1.0897149 1.0940076 1.0941355 1.094278\n",
      " 1.0963126 1.0972834 1.0844724 1.0883565 1.090994  1.0926628 1.1016403\n",
      " 1.0958278 1.0931876 1.0909605 1.0953268 1.0919482 1.0913243 1.0913264\n",
      " 1.0919974 1.0930789 1.0855267 1.0921041 1.0956745 1.0916009 1.0942085\n",
      " 1.0969819 1.0845554 1.0978092 1.0921203 1.0971173 1.0919118 1.0956101\n",
      " 1.0926898 1.0949612 1.09483   1.0944891 1.1440221 1.088552  1.0929337\n",
      " 1.0960237 1.0853729 1.0903779 1.0973976 1.0817537 1.0978427 1.0926377\n",
      " 1.0914359 1.0878367 1.0922416 1.0951866 1.0842489 1.0932425 1.0900248\n",
      " 1.0983973 1.0962813 1.0974978 1.0959771 1.0903969 1.094553  1.0925477\n",
      " 1.0986073 1.0871524 1.0988123 1.0928442 1.0905201 1.0891973 1.0932379\n",
      " 1.0945574 1.095033  1.0968299 1.0844359 1.0884495 1.0961318 1.0946376\n",
      " 1.0928681 1.0972779 1.0948186 1.0862566 1.0992275 1.0868033 1.0978631\n",
      " 1.0967021 1.0889324 1.0919938 1.09178   1.0944803 1.0885861 1.1035775\n",
      " 1.0954149 1.0909576 1.0891151 1.0946982 1.0958445 1.0873361 1.0924464\n",
      " 1.0910071 1.0926639 1.0987251 1.0947273 1.0970464 1.0897393 1.0915879\n",
      " 1.0977536 1.0952402 1.088566  1.0956908 1.1023355 1.0882931 1.0923021\n",
      " 1.0937765 1.0894401 1.0916754 1.095248  1.0935656 1.0889019 1.0947487\n",
      " 1.0916463 1.0979031 1.0933037 1.1170505 1.0815871 1.0822741 1.0937653\n",
      " 1.084085  1.0897415 1.0896826 1.0982145 1.0917673 1.0886328 1.0968345\n",
      " 1.0902841 1.0992829 1.0947378 1.0845798 1.0815493 1.091948  1.0905149\n",
      " 1.0961056 1.0913335 1.0830106 1.0916338 1.0877923 1.098313  1.0965879\n",
      " 1.0978802 1.0834916 1.089907  1.0894908 1.0929245 1.0937684 1.0957147\n",
      " 1.0923463 1.0946629 1.0985193 1.0896418 1.0960078 1.0891714 1.1004086\n",
      " 1.0945879 1.0888984 1.0926423 1.0962309 1.1002988 1.096024  1.0922434\n",
      " 1.0865963 1.0894259 1.0862048 1.084335  1.101017  1.087417  1.096208\n",
      " 1.0886165 1.0952182 1.0888731 1.0962336 1.0922242 1.0891501 1.0991542\n",
      " 1.097247  1.096666  1.0879326 1.0984991 1.092508  1.0890397 1.1001542\n",
      " 1.0989822 1.0765681 1.0954392 1.0984671 1.088952  1.0947407 1.0969124\n",
      " 1.0889162 1.0959616 1.0927491 1.0979714 1.0938306 1.090409  1.0938823\n",
      " 1.0953054 1.0919207 1.0850912 1.1043398 1.0923828 1.0981634 1.0882597\n",
      " 1.0871578 1.0949882 1.0764761 1.0922666 1.0935031 1.0894433 1.0952362\n",
      " 1.0923973 1.0948822 1.0816454 1.0872341 1.099245  1.0887105 1.0841211\n",
      " 1.0908594 1.0892692 1.0945542 1.0919378 1.0949295 1.089772  1.0905465\n",
      " 1.0950938 1.0931735 1.0997912 1.0924098 1.0917139 1.098903  1.0903246\n",
      " 1.078519  1.0886943 1.0917397 1.0934179 1.091368  1.0926023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0776807 1.0562396 1.0709846 1.0747758 1.0594249 1.0638448 1.0694392\n",
      " 1.0673146 1.0554793 1.0806065 1.0757731 1.0563568 1.0654907 1.0559552\n",
      " 1.0392295 1.0855463 1.0632952 1.0821278 1.0688456 1.0575495 1.0753996\n",
      " 1.0406826 1.0788616 1.0716242 1.0458146 1.054126  1.0630984 1.0736521\n",
      " 1.0684865 1.0890788 1.0461212 1.0544368 1.0899209 1.0584177 1.0435611\n",
      " 1.0602684 1.0772271 1.0608697 1.05227   1.044356  1.0740852 1.0469285\n",
      " 1.0494293 1.0679476 1.0816464 1.0757308 1.0773044 1.0572855 1.0659474\n",
      " 1.0945218 1.0526652 1.0618229 1.0482908 1.0442268 1.060491  1.071947\n",
      " 1.061386  1.0667737 1.0601712 1.0662309 1.059475  1.0758512 1.0549481\n",
      " 1.0927768 1.0756872 1.0774115 1.0587314 1.0613596 1.0783077 1.0707403\n",
      " 1.0714321 1.0761348 1.0471958 1.072853  1.0620315 1.0590316 1.0635751\n",
      " 1.0562791 1.0893364 1.0491595 1.0764222 1.0721862 1.0544927 1.0551261\n",
      " 1.0612748 1.0784013 1.0752277 1.072279  1.0840732 1.0544962 1.0381606\n",
      " 1.056824  1.0411514 1.0692651 1.076923  1.0764621 1.0615808 1.061653\n",
      " 1.0640929 1.0469162 1.0402464 1.076803  1.08167   1.0640339 1.0625136\n",
      " 1.0642744 1.0624543 1.1047802 1.0707519 1.0637164 1.0846919 1.0758271\n",
      " 1.069224  1.0540364 1.0453411 1.0832155 1.0700318 1.0708616 1.0687491\n",
      " 1.0749974 1.0708969 1.0699893 1.0631994 1.0777961 1.0731128 1.0719594\n",
      " 1.0605327 1.0764387 1.0657706 1.0823085 1.0721402 1.0584267 1.0475674\n",
      " 1.0836902 1.0643804 1.0761621 1.06233   1.0627198 1.0720667 1.0590699\n",
      " 1.052394  1.081583  1.0985442 1.0830703 1.0740967 1.0723472 1.0805795\n",
      " 1.0907762 1.0741441 1.080313  1.0560558 1.0515962 1.0853792 1.0856689\n",
      " 1.092031  1.0789284 1.0695698 1.0641849 1.0660479 1.0631989 1.0673794\n",
      " 1.0466064 1.0853642 1.0911186 1.0512145 1.0664847 1.0870734 1.0673059\n",
      " 1.0559921 1.0615301 1.0692017 1.056771  1.0670226 1.0646716 1.0487152\n",
      " 1.0491351 1.077911  1.0650929 1.0767138 1.0730364 1.0708038 1.0771346\n",
      " 1.0743717 1.0706334 1.0405797 1.0677493 1.040054  1.0696611 1.0703114\n",
      " 1.072515  1.0718558 1.0835047 1.0680615 1.0654883 1.0647012 1.0819979\n",
      " 1.0464553 1.0443712 1.0512474 1.0529995 1.0735958 1.0728443 1.0644803\n",
      " 1.0639701 1.0427297 1.073419  1.0885738 1.0622015 1.0592029 1.0959312\n",
      " 1.0554271 1.0800035 1.0491672 1.0556909 1.0593836 1.0736797 1.0627983\n",
      " 1.0746703 1.0903721 1.0556407 1.0762138 1.0519689 1.0634043 1.0554423\n",
      " 1.0593529 1.0653167 1.0660992 1.0458407 1.0686027 1.0565991 1.0626159\n",
      " 1.0586413 1.0611769 1.0699612 1.0611757 1.0561714 1.0701302 1.0571537\n",
      " 1.0498765 1.0655308 1.0644691 1.0673715 1.0511167 1.082564  1.066576\n",
      " 1.0613549 1.0790867 1.0832925 1.0520546 1.0694237 1.075564  1.0592846\n",
      " 1.0440787 1.0730745 1.093504  1.0736878 1.0636569 1.0857103 1.0956635\n",
      " 1.0781804 1.0716588 1.0769637 1.0647061 1.0566783 1.0624778 1.0600991\n",
      " 1.0603245 1.0589871 1.0449779 1.049283  1.056005  1.0447897 1.083725\n",
      " 1.0585966 1.0639749 1.0813179 1.0552231 1.063338  1.0893642 1.0692778\n",
      " 1.0506886 1.0870117 1.059741  1.054274  1.0662887 1.0746531 1.0685903\n",
      " 1.0650237 1.0635654 1.0628002 1.0851742 1.0738381 1.0706527 1.0442435\n",
      " 1.0709565 1.0625448 1.072065  1.053923  1.0508163 1.0810132 1.0868638\n",
      " 1.0648757 1.0575551 1.0548619 1.0721982 1.0878468 1.0640455 1.0734528\n",
      " 1.065756  1.0580919 1.0494242 1.0841489 1.073849  1.0543677 1.0400816\n",
      " 1.0678222 1.0622823 1.0494008 1.0930136 1.0435208 1.0659602 1.0571007\n",
      " 1.0808539 1.0620751 1.0631733 1.0504506 1.0472238 1.0815777 1.0699537\n",
      " 1.0464505 1.066022  1.0895904 1.0669734 1.0726095 1.0585761 1.065124\n",
      " 1.0505846 1.0481769 1.0710157 1.0845921 1.0600705 1.0494752 1.0656006\n",
      " 1.0678548 1.0581347 1.0537306 1.0551287 1.0512142 1.0719147 1.0856875\n",
      " 1.085103  1.0689523 1.0475876 1.0622482 1.0717479 1.0640602 1.0750746\n",
      " 1.0575366 1.0703778 1.0440061 1.0391124 1.0599099 1.0558479 1.0647875\n",
      " 1.0693163 1.0625536 1.0735275 1.0529181 1.0712997 1.0722668 1.0712415\n",
      " 1.0615513 1.0384505 1.0400457 1.0412798 1.0742111 1.0664241 1.0478852\n",
      " 1.0797112 1.070039  1.0608251 1.0769751 1.0609298 1.083081  1.0589864\n",
      " 1.0932271 1.0457141 1.0834646 1.0566524 1.069161  1.0568361 1.0541619\n",
      " 1.0409943 1.0620607 1.0646745 1.0489492 1.0630705 1.0717443 1.0800362\n",
      " 1.0641464 1.0677838 1.0376925 1.0537635 1.0700924 1.0779805 1.060054\n",
      " 1.053046  1.0637745 1.0666779 1.0651451 1.0491285 1.0694296 1.0704741\n",
      " 1.0844711 1.0733877 1.0439494 1.0606625 1.0817466 1.0528885 1.0737932\n",
      " 1.0755373 1.0651846 1.0557865 1.0632722 1.0786874 1.0633755 1.0472665\n",
      " 1.0874388 1.0671012 1.0488942 1.0917494 1.0636562 1.0584162 1.0615108\n",
      " 1.0587456 1.0349137 1.0625705 1.0685279 1.0840966 1.062278  1.0722198\n",
      " 1.0589496 1.0670984 1.0559373 1.0652405 1.0636246 1.0798658 1.0809985\n",
      " 1.0431232 1.0537692 1.0514734 1.0576999 1.0658283 1.0461193 1.0905665\n",
      " 1.0589094 1.0581186 1.0541054 1.0521421 1.0842916 1.0575889 1.0842957\n",
      " 1.069496  1.0572522 1.063937  1.0954754 1.0792718 1.0492845 1.048523\n",
      " 1.0686331 1.0613939 1.07315   1.0598923 1.0790235 1.0602484 1.0651405\n",
      " 1.0725002 1.0582925 1.0679913 1.0735307 1.0427413 1.0693909 1.0509\n",
      " 1.0744709 1.0760072 1.0521667 1.0846881 1.072595  1.0750786 1.0798669\n",
      " 1.0872461 1.0749384 1.0694283 1.0617366 1.0704542 1.0659404 1.055358\n",
      " 1.0723782 1.062602  1.0512644 1.0515461 1.0570103 1.0765432 1.0604258\n",
      " 1.0469239 1.0691473 1.0698541 1.0953572 1.0452857 1.0464615 1.068773\n",
      " 1.0880197 1.0388551 1.0611453 1.064534  1.0605576 1.0506324 1.0749025\n",
      " 1.0843711 1.0671338 1.0653915 1.0457352 1.0621263 1.0385177 1.0744326\n",
      " 1.0714588 1.0472742 1.0797018 1.0622294 1.0704161 1.0917282 1.0654454\n",
      " 1.0726846 1.0591202 1.0643895 1.0584508 1.0718621 1.0904528 1.0730953\n",
      " 1.0636544 1.0529791 1.0692403 1.0762422 1.0559735 1.0893452 1.0799557\n",
      " 1.0574067 1.0806026 1.0890384 1.0507697 1.07155   1.057224  1.0893656\n",
      " 1.0638794 1.0748842 1.0709622 1.0519179 1.0686368 1.0670794 1.0537046\n",
      " 1.0659101 1.0682827 1.084974  1.0709295 1.0677618 1.0734475 1.0718068\n",
      " 1.0376126 1.074189  1.0743256 1.0633035 1.0831157 1.050695  1.0406867\n",
      " 1.0736698 1.0617267 1.089798  1.0748576 1.091348  1.096921  1.0896076\n",
      " 1.0839176 1.0712662 1.0869453 1.0767028 1.0533353 1.0859939 1.0632664\n",
      " 1.0651453 1.0675302 1.0480187 1.0626098 1.0788786 1.047354  1.0698342\n",
      " 1.0631785 1.065752  1.0657377 1.0680208 1.0758178 1.0884767 1.0760336\n",
      " 1.0494127 1.087746  1.0803138 1.0663383 1.0680068 1.0864094 1.0597974\n",
      " 1.0900402 1.0660359 1.0774537 1.0540633 1.0610055 1.0705376 1.0769798\n",
      " 1.0851146 1.0590429 1.0629413 1.0647186 1.0916892 1.0614868 1.064043\n",
      " 1.0656617 1.069926  1.0711905 1.060108  1.0602759 1.0530579 1.0642396\n",
      " 1.0851308 1.0829393 1.061497  1.077401  1.0639331 1.0681783 1.0655282\n",
      " 1.0776285 1.050319  1.0357047 1.0735533 1.0553995 1.0794244 1.0572839\n",
      " 1.0500398 1.0652299 1.0927037 1.0582595 1.0405661 1.0708476 1.0840778\n",
      " 1.0699383 1.0756818 1.0826087 1.063064  1.0679214 1.0640159 1.0532273\n",
      " 1.0720404 1.0802722 1.0567205 1.0520338 1.1007583 1.0872109 1.066274\n",
      " 1.08476   1.0601233 1.0793693 1.0838585 1.0649109 1.0652721 1.0434755\n",
      " 1.0768621 1.067786  1.0734855 1.0508935 1.0589395 1.0618405 1.046236\n",
      " 1.0756226 1.055964  1.0903885 1.0733429 1.082318  1.0681486 1.060247\n",
      " 1.06654   1.0613492 1.057774  1.0722102 1.0793082 1.0766393 1.0561907\n",
      " 1.0852946 1.0622007 1.0750242 1.0578957 1.0411633 1.0681908 1.059857\n",
      " 1.0601037 1.0693957 1.0713637 1.0513767 1.0730878 1.0283629 1.0545349\n",
      " 1.0612406 1.0646241 1.0740429 1.0518    1.08213   1.0996401 1.0689352\n",
      " 1.091587  1.0505854 1.0487761 1.0497626 1.0489401 1.0897883 1.0836904\n",
      " 1.052874  1.0632117 1.0717565 1.0582728 1.0465988 1.061832  1.0872467\n",
      " 1.0721501 1.070762  1.0551693 1.0715196 1.0592191 1.0892256 1.0464268\n",
      " 1.0609823 1.0660986 1.0479121 1.0817302 1.0657917 1.0884567 1.0775766\n",
      " 1.0804675 1.0551344 1.0609256 1.0455987 1.0521594 1.0678158 1.0532014\n",
      " 1.0533749 1.0519722 1.0687673 1.0558197 1.0441111 1.0532544 1.0750023\n",
      " 1.0710112 1.0536288 1.0737424 1.054177  1.052519  1.074189  1.0707731\n",
      " 1.0725708 1.0691271 1.0644284 1.0626922 1.0691861 1.0658371 1.0820218\n",
      " 1.0645974 1.0649134 1.0692816 1.0607536 1.0564295 1.0820462 1.0958197\n",
      " 1.0404706 1.0701628 1.0666602 1.0534936 1.0568105 1.0715829 1.0617009\n",
      " 1.0765889 1.0463198 1.0490855 1.0549774 1.0687356 1.0684062 1.0751774\n",
      " 1.0633217 1.0669578 1.0751218 1.0810272 1.0512239 1.0540571 1.0858672\n",
      " 1.0785741 1.086106  1.0471679 1.0682381 1.0703195 1.0589356 1.071759\n",
      " 1.0937481 1.0421189 1.0730531 1.061463  1.0519816 1.0776788 1.057996\n",
      " 1.0657132 1.0650071 1.0693147 1.0712225 1.0602076 1.0744209 1.0434475\n",
      " 1.08172   1.0451809 1.0775054 1.056541  1.0765684 1.0589132 1.0579585\n",
      " 1.0714798 1.0573533 1.0828968 1.0460981 1.0736996 1.0884764 1.0793113\n",
      " 1.0796189 1.0713954 1.0716404 1.0641892 1.0708929 1.056751  1.0853348\n",
      " 1.0356447 1.0411757 1.043177  1.0662024 1.0702685 1.057081  1.0547127\n",
      " 1.073082  1.0952864 1.0824078 1.0475694 1.0741844 1.0668019 1.0475047\n",
      " 1.0731174 1.0532877 1.1016499 1.0754051 1.0787654 1.0833877 1.0625484\n",
      " 1.0748453 1.0650749 1.0698277 1.0613105 1.0477996 1.0846313 1.066592\n",
      " 1.0578189 1.0740604 1.1052642 1.0541234 1.0855255 1.0650718 1.0798411\n",
      " 1.0575308 1.0827403 1.0852042 1.0816422 1.0558085 1.0489235 1.0786594\n",
      " 1.0648599 1.0355773 1.0466753 1.0487868 1.0670158 1.0584049 1.0457549\n",
      " 1.0780479 1.0599756 1.0587299 1.0577185 1.070663  1.0467294 1.0463462\n",
      " 1.065559  1.0787134 1.0558945 1.0618063 1.073259  1.0636672 1.0737865\n",
      " 1.0513368 1.0636154 1.0572174 1.0770237 1.0593741 1.0652771 1.066035\n",
      " 1.0565685 1.065819  1.0546784 1.0723537 1.0645887 1.0376413 1.0575931\n",
      " 1.0730302 1.0785912 1.073041  1.0650764 1.0763042 1.0531075 1.0502529\n",
      " 1.057368  1.0795794 1.0628313 1.095826  1.0631611 1.0740758 1.045625\n",
      " 1.0467963 1.0748835 1.0599065 1.0643758 1.042947  1.0744166 1.0523115\n",
      " 1.086429  1.077043  1.0567548 1.0786525 1.0882844 1.075647  1.056956\n",
      " 1.050776  1.0557297 1.0504744 1.0621853 1.0749778 1.0566881 1.0782288\n",
      " 1.0788989 1.0617706 1.0594212 1.076022  1.0615648 1.0720582 1.0742747\n",
      " 1.0802858 1.0680355 1.0989909 1.0749863 1.0501077 1.0644855 1.0448976\n",
      " 1.0538203 1.0490761 1.0358169 1.0598613 1.0726807 1.0467246 1.0833489\n",
      " 1.0550611 1.0630801 1.0522048 1.0369049 1.0802798 1.0736883 1.0679047\n",
      " 1.0518467 1.041445  1.0464479 1.0680428 1.0672812 1.070737  1.0508924\n",
      " 1.0650656 1.0477445 1.0628672 1.0521839 1.0858705 1.0609426 1.060721\n",
      " 1.0448016 1.0561197 1.06327   1.0715894 1.0748061 1.055866 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6442213 4.623729  4.686319  4.6163235 4.655056  4.674011  4.637671\n",
      " 4.689971  4.6621027 4.6324863 4.666173  4.641423  4.6316876 4.6347294\n",
      " 4.642106  4.6500964 4.6505213 4.6512403 4.6526766 4.6189194 4.6761622\n",
      " 4.629012  4.6769295 4.6643257 4.6639624 4.6564984 4.681378  4.6607943\n",
      " 4.6014104 4.6431184 4.685634  4.6280584 4.627391  4.65817   4.664446\n",
      " 4.6684046 4.654108  4.6175117 4.641708  4.6347446 4.6553965 4.645027\n",
      " 4.6212964 4.704562  4.624331  4.658491  4.6186576 4.673847  4.6502876\n",
      " 4.5920706 4.6536107 4.665118  4.643426  4.685197  4.632352  4.6719604\n",
      " 4.660183  4.691877  4.649454  4.6615124 4.642329  4.6086445 4.650814\n",
      " 4.651055  4.6473064 4.681642  4.603773  4.673092  4.691448  4.658743\n",
      " 4.5951076 4.6373196 4.5968246 4.6673746 4.6361523 4.630529  4.6500297\n",
      " 4.632832  4.660911  4.6307306 4.622882  4.646167  4.6483603 4.66522\n",
      " 4.6246495 4.6127825 4.6538486 4.657163  4.648596  4.6517105 4.6304073\n",
      " 4.6247377 4.662838  4.6551247 4.6441383 4.586515  4.589794  4.6223636\n",
      " 4.68645   4.6722736 4.6504254 4.65316   4.70677   4.698411  4.6285048\n",
      " 4.633051  4.597596  4.6451454 4.5995264 4.617976  4.6747255 4.6589694\n",
      " 4.6341934 4.6379185 4.670103  4.660203  4.6791267 4.6481395 4.6488743\n",
      " 4.681365  4.6351457 4.653886  4.6354647 4.648433  4.6621222 4.688585\n",
      " 4.691073  4.6385136 4.633197  4.656399  4.641268  4.65532   4.6602473\n",
      " 4.65272   4.6539288 4.636796  4.6313543 4.6557794 4.6693544 4.6722426\n",
      " 4.677329  4.699812  4.6220093 4.6890736 4.660301  4.622731  4.6592855\n",
      " 4.6908092 4.6520357 4.559473  4.6823854 4.631384  4.632554  4.702413\n",
      " 4.6365647 4.677546  4.5953054 4.6846237 4.6418657 4.651612  4.631081\n",
      " 4.612955  4.628797  4.627159  4.6396184 4.63916   4.618232  4.637841\n",
      " 4.6127005 4.6481376 4.640137  4.635581  4.635825  4.6460457 4.609303\n",
      " 4.6710377 4.653063  4.632237  4.6783085 4.6537037 4.6717634 4.5858088\n",
      " 4.6405416 4.662763  4.6249228 4.630219  4.6768746 4.6508393 4.641754\n",
      " 4.658226  4.6314077 4.6169405 4.598627  4.6205435 4.6655936 4.637267\n",
      " 4.63212   4.6374903 4.63968   4.66409   4.666649  4.7001076 4.6268973\n",
      " 4.6598053 4.630948  4.668894  4.66808   4.622899  4.617421  4.628833\n",
      " 4.649336  4.6410904 4.6601963 4.6670923 4.644066  4.6735716 4.638966\n",
      " 4.666355  4.6038384 4.6516895 4.6365595 4.6144433 4.624874  4.6741037\n",
      " 4.700775  4.652581  4.652088  4.6820874 4.64      4.6247845 4.599075\n",
      " 4.660155  4.675528  4.6313415 4.684677  4.6033926 4.5984783 4.68003\n",
      " 4.6369047 4.657905  4.679395  4.6554937 4.6255445 4.65899   4.625512\n",
      " 4.6289425 4.637842  4.652985  4.6289005 4.596074  4.694412  4.6692944\n",
      " 4.633065  4.6626806 4.6190767 4.6563067 4.640891  4.65518   4.645157\n",
      " 4.687771  4.6188574 4.675729  4.6456203 4.630774  4.6239367 4.689642\n",
      " 4.6544476 4.660921  4.653214  4.694069  4.6167946 4.6685762 4.6585054\n",
      " 4.672396  4.6374817 4.657402  4.610596  4.6414204 4.672667  4.6421223\n",
      " 4.693315  4.6406994 4.648238  4.6507115 4.6054754 4.6635365 4.634859\n",
      " 4.633217  4.631069  4.634456  4.660379  4.7000647 4.6277456 4.6402664\n",
      " 4.6250343 4.6719303 4.6583576 4.6286716 4.6790743 4.6762056 4.6555023\n",
      " 4.647252  4.686371  4.6514425 4.671326  4.676347  4.655218  4.66934\n",
      " 4.6525173 4.6562514 4.6514683 4.65519   4.6849103 4.6809883 4.683852\n",
      " 4.6646423 4.6428585 4.6296363 4.63474   4.6302323 4.60954   4.671795\n",
      " 4.6483507 4.623283  4.6186094 4.6445684 4.6737843 4.660948  4.6942368\n",
      " 4.6985435 4.648424  4.635255  4.615683  4.647315  4.6021214 4.6182203\n",
      " 4.6387215 4.663636  4.668875  4.6454334 4.6579676 4.6801825 4.6754932\n",
      " 4.600912  4.6659126 4.6681786 4.6772203 4.620447  4.660278  4.661692\n",
      " 4.6816254 4.6611977 4.6856356 4.5938444 4.6823087 4.627769  4.640629\n",
      " 4.7216697 4.637832  4.6438704 4.672413  4.6577187 4.648262  4.661596\n",
      " 4.616617  4.633213  4.603714  4.6492767 4.6502476 4.6433444 4.6953287\n",
      " 4.6750593 4.659601  4.642123  4.6594696 4.640457  4.632136  4.635664\n",
      " 4.6847115 4.6293707 4.6845384 4.6306543 4.6407804 4.6694326 4.658186\n",
      " 4.6634226 4.628071  4.668396  4.5833073 4.624546  4.633307  4.5803633\n",
      " 4.6146293 4.6343904 4.6757402 4.640808  4.644516  4.6225634 4.6238546\n",
      " 4.6738167 4.6901894 4.703856  4.658867  4.644048  4.622732  4.6423664\n",
      " 4.66792   4.637143  4.7392435 4.648154  4.688553  4.687245  4.615759\n",
      " 4.646218  4.6724195 4.637527  4.6971564 4.662289  4.6632967 4.6731825\n",
      " 4.6505027 4.674877  4.660997  4.656335  4.5980754 4.6723843 4.607016\n",
      " 4.6451616 4.677684  4.6018953 4.638114  4.6739616 4.693285  4.665884\n",
      " 4.648081  4.623809  4.6624613 4.668978  4.614753  4.6359067 4.6579275\n",
      " 4.6216154 4.670915  4.6894875 4.6916847 4.6816216 4.6508265 4.6598725\n",
      " 4.67655   4.653529  4.598398  4.64283   4.6168423 4.6746454 4.5994596\n",
      " 4.694359  4.593097  4.647574  4.66731   4.6247983 4.6573715 4.6518335\n",
      " 4.6349754 4.679674  4.696478  4.643064  4.636327  4.6955037 4.6174374\n",
      " 4.588068  4.6468897 4.6459565 4.6455007 4.698015  4.6301746 4.619216\n",
      " 4.6579137 4.617736  4.622117  4.692128  4.673932  4.649151  4.626668\n",
      " 4.6847305 4.6571836 4.6327333 4.6248136 4.630662  4.6488285 4.622075\n",
      " 4.6059675 4.6548734 4.6530104 4.6261263 4.598022  4.665299  4.6564817\n",
      " 4.6748195 4.616866  4.6510963 4.682895  4.649474  4.6739397 4.6024046\n",
      " 4.6490498 4.622993  4.677144  4.6241455 4.617755  4.64185   4.6837726\n",
      " 4.649698  4.6115427 4.6003017 4.618165  4.592599  4.618342  4.660561\n",
      " 4.667981  4.6722383 4.635559  4.6842637 4.629963  4.6927643 4.671561\n",
      " 4.599512  4.665884  4.6150193 4.6231766 4.6862626 4.6449666 4.6247764\n",
      " 4.626462  4.626001  4.6271524 4.678918  4.72595   4.653535  4.6580763\n",
      " 4.694509  4.64528   4.6828365 4.6672106 4.648608  4.6441126 4.6691484\n",
      " 4.6403437 4.6903057 4.6160765 4.7211785 4.6843066 4.6340313 4.620604\n",
      " 4.669014  4.6574774 4.6136246 4.652889  4.671129  4.6340685 4.655889\n",
      " 4.693627  4.625597  4.610089  4.632923  4.686432  4.6596265 4.672615\n",
      " 4.6328917 4.655699  4.6631255 4.649676  4.694165  4.668064  4.6750765\n",
      " 4.61172   4.6797767 4.6225424 4.674007  4.6301546 4.6232443 4.6470904\n",
      " 4.6612425 4.6444225 4.638246  4.647833  4.667336  4.5810914 4.6207056\n",
      " 4.7191863 4.649927  4.680703  4.6456537 4.6721478 4.67926   4.662664\n",
      " 4.704089  4.6398335 4.6574526 4.6596403 4.646312  4.6696253 4.6508117\n",
      " 4.656269  4.631979  4.652652  4.6827993 4.658572  4.6394405 4.6458464\n",
      " 4.6921215 4.644603  4.6642017 4.619788  4.695064  4.6847916 4.6546173\n",
      " 4.6478415 4.6677217 4.709107  4.609274  4.6599455 4.6326866 4.687583\n",
      " 4.6671305 4.6542425 4.6930747 4.651046  4.691207  4.64285   4.6472673\n",
      " 4.6469584 4.6494274 4.6235995 4.627613  4.6153126 4.665821  4.6562567\n",
      " 4.6446695 4.648178  4.624856  4.642885  4.609686  4.6755834 4.674582\n",
      " 4.661642  4.6108828 4.669038  4.65327   4.606801  4.651955  4.6332097\n",
      " 4.6642303 4.6377473 4.6614614 4.654977  4.674301  4.6275463 4.6579742\n",
      " 4.609679  4.6403904 4.655427  4.652228  4.669879  4.6858234 4.609683\n",
      " 4.6417    4.6722736 4.6478796 4.674471  4.708584  4.6543465 4.6621923\n",
      " 4.630328  4.6169086 4.6514015 4.615681  4.621833  4.613037  4.6293907\n",
      " 4.656415  4.7167645 4.646858  4.685769  4.606642  4.6336784 4.658547\n",
      " 4.6958246 4.6342106 4.6326103 4.6156316 4.630789  4.6428013 4.6165643\n",
      " 4.64096   4.6208143 4.6408377 4.6839476 4.6462693 4.689558  4.614894\n",
      " 4.686424  4.653036  4.641577  4.6737585 4.66455   4.6543255 4.6910486\n",
      " 4.674904  4.646682  4.657611  4.6485376 4.674612  4.69993   4.6868844\n",
      " 4.6401854 4.6562867 4.6186347 4.6759176 4.6598167 4.6469135 4.6321354\n",
      " 4.606941  4.636773  4.697747  4.6287007 4.633374  4.679783  4.634202\n",
      " 4.617495  4.6572433 4.7179937 4.650943  4.6669607 4.6447754 4.6444\n",
      " 4.6536465 4.5982494 4.661622  4.65973   4.667902  4.637772  4.6369247\n",
      " 4.6429315 4.657667  4.6423674 4.6945286 4.658116  4.644227  4.6635313\n",
      " 4.6825786 4.6278086 4.697238  4.6798887 4.6505575 4.595172  4.6440525\n",
      " 4.6338186 4.6204433 4.6271014 4.6656    4.6548886 4.665362  4.6226854\n",
      " 4.6845875 4.6576357 4.6562877 4.683053  4.6452494 4.700148  4.6259136\n",
      " 4.6404467 4.6238346 4.61954   4.669393  4.705026  4.6113586 4.6658063\n",
      " 4.651561  4.7220926 4.666732  4.6898813 4.6913495 4.626668  4.651016\n",
      " 4.6486616 4.66598   4.658723  4.636257  4.703208  4.659066  4.580428\n",
      " 4.6486664 4.632146  4.580576  4.6351004 4.6590214 4.600177  4.6530027\n",
      " 4.6749    4.6463037 4.65444   4.645315  4.6510706 4.647611  4.6649394\n",
      " 4.6174507 4.6364813 4.656743  4.662675  4.6047916 4.6706347 4.6334286\n",
      " 4.6316204 4.6726027 4.6209984 4.663287  4.6122513 4.6683946 4.669775\n",
      " 4.610383  4.6530056 4.5781436 4.62393   4.639398  4.632741  4.6364794\n",
      " 4.656208  4.660368  4.659923  4.678478  4.6494675 4.6771927 4.630946\n",
      " 4.6908436 4.6469326 4.599908  4.6558485 4.6487975 4.6702976 4.67729\n",
      " 4.7011547 4.677185  4.620018  4.628353  4.6694813 4.6313276 4.665555\n",
      " 4.6143255 4.652053  4.6265755 4.6371784 4.632206  4.667306  4.636982\n",
      " 4.6315813 4.663378  4.618792  4.6321197 4.644212  4.6352262 4.6398945\n",
      " 4.65512   4.6300898 4.6860943 4.5911245 4.652158  4.6418076 4.6373086\n",
      " 4.6181517 4.633789  4.677862  4.6921253 4.642462  4.6888785 4.674958\n",
      " 4.661293  4.641609  4.670376  4.695881  4.6370726 4.6523495 4.6457415\n",
      " 4.669161  4.655922  4.60143   4.6319075 4.6243496 4.664833  4.6582584\n",
      " 4.687599  4.659919  4.628689  4.6557226 4.645211  4.6869698 4.619733\n",
      " 4.626777  4.6466637 4.611274  4.6155777 4.700532  4.6464195 4.7002945\n",
      " 4.650103  4.629305  4.641469  4.640115  4.67614   4.689022  4.643771\n",
      " 4.630336  4.655381  4.6364117 4.6657095 4.638717  4.6870294 4.6727276\n",
      " 4.6759214 4.6772575 4.663949  4.632907  4.6579094 4.6503353 4.684738\n",
      " 4.68888   4.6523314 4.662096  4.685441  4.675907  4.6316566 4.6530075\n",
      " 4.6581593 4.6766815 4.6459174 4.61553   4.6027675 4.6146264 4.685918\n",
      " 4.6372404 4.6433535 4.6441946 4.616354  4.6194935 4.619439  4.645314\n",
      " 4.6584272 4.6932583 4.6805687 4.6589503 4.6582937 4.6399794 4.6596336\n",
      " 4.628716  4.694113  4.6452074 4.6286793 4.6645274 4.665718  4.727751\n",
      " 4.6253843 4.664859  4.6337366 4.6355743 4.6616306 4.72073   4.6493917\n",
      " 4.6529274 4.62853   4.6741095 4.641323  4.669322  4.6191196 4.6451063\n",
      " 4.6578436 4.656081  4.693223  4.6495037 4.6524262 4.651005  4.6410093\n",
      " 4.6703143 4.66877   4.659026  4.5881777 4.7012863 4.6045265 4.65586\n",
      " 4.6371994 4.620078  4.6230946 4.6044397 4.632917  4.648426  4.64215\n",
      " 4.608864  4.651129  4.6625996 4.659389  4.644512  4.6999397]\n"
     ]
    }
   ],
   "source": [
    "#学習パラメータの取得\n",
    "save_model_path = \"/Users/nagaiyuma/Desktop/maindata/model_cnn.h5\"\n",
    "model = load_model(save_model_path)\n",
    "\n",
    "#predict_x0の確認\n",
    "print(model.predict(test_x).reshape(1000,3).T[0])\n",
    "print(model.predict(test_x).reshape(1000,3).T[1])\n",
    "print(model.predict(test_x).reshape(1000,3).T[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09284683001041412 0.006017291430585471\n",
      "0.5068420162916185 0.3007633356328077\n",
      "0.34991920471191407 0.026563036661073552\n"
     ]
    }
   ],
   "source": [
    "abs_error = abs(np.array(model.predict(test_x).reshape(1000,3)).T[0]- tx_test.T[0])\n",
    "ave_abs = np.average(abs_error)\n",
    "std_abs = np.std(abs_error)\n",
    "print(ave_abs, std_abs)\n",
    "abs_error = abs(np.array(model.predict(test_x).reshape(1000,3)).T[1]- tx_test.T[1])\n",
    "ave_abs = np.average(abs_error)\n",
    "std_abs = np.std(abs_error)\n",
    "print(ave_abs, std_abs)\n",
    "abs_error = abs(np.array(model.predict(test_x).reshape(1000,3)).T[2]- tx_test.T[2])\n",
    "ave_abs = np.average(abs_error)\n",
    "std_abs = np.std(abs_error)\n",
    "print(ave_abs, std_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 20,503\n",
      "Trainable params: 20,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 2s 167us/step - loss: 2.2099 - mae: 0.9234 - val_loss: 0.1382 - val_mae: 0.2583\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s 88us/step - loss: 0.3075 - mae: 0.4350 - val_loss: 0.1293 - val_mae: 0.2427\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s 85us/step - loss: 0.2697 - mae: 0.4027 - val_loss: 0.1264 - val_mae: 0.2374\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.2553 - mae: 0.3887 - val_loss: 0.1350 - val_mae: 0.2521\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.2405 - mae: 0.3725 - val_loss: 0.1459 - val_mae: 0.2738\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.2305 - mae: 0.3619 - val_loss: 0.1271 - val_mae: 0.2346\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.2214 - mae: 0.3519 - val_loss: 0.1566 - val_mae: 0.2982\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.2191 - mae: 0.3476 - val_loss: 0.1326 - val_mae: 0.2503\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.2101 - mae: 0.3386 - val_loss: 0.1203 - val_mae: 0.2180\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.2056 - mae: 0.3342 - val_loss: 0.1298 - val_mae: 0.2454\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.2032 - mae: 0.3308 - val_loss: 0.1212 - val_mae: 0.2209\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 0.1993 - mae: 0.3271 - val_loss: 0.1323 - val_mae: 0.2507\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s 75us/step - loss: 0.2001 - mae: 0.3264 - val_loss: 0.1202 - val_mae: 0.2185\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.1962 - mae: 0.3227 - val_loss: 0.1243 - val_mae: 0.2293\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1953 - mae: 0.3220 - val_loss: 0.1173 - val_mae: 0.2091\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.1909 - mae: 0.3177 - val_loss: 0.1178 - val_mae: 0.2100\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 0.1892 - mae: 0.3157 - val_loss: 0.1172 - val_mae: 0.2061\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.1875 - mae: 0.3136 - val_loss: 0.1206 - val_mae: 0.2219\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s 111us/step - loss: 0.1863 - mae: 0.3125 - val_loss: 0.1294 - val_mae: 0.2497\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.1856 - mae: 0.3121 - val_loss: 0.1403 - val_mae: 0.2738\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s 89us/step - loss: 0.1803 - mae: 0.3065 - val_loss: 0.1161 - val_mae: 0.2057\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.1829 - mae: 0.3088 - val_loss: 0.1213 - val_mae: 0.2250\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1784 - mae: 0.3043 - val_loss: 0.1169 - val_mae: 0.2077\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1764 - mae: 0.3023 - val_loss: 0.1218 - val_mae: 0.2258\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 0.1752 - mae: 0.3004 - val_loss: 0.1388 - val_mae: 0.2735\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.1758 - mae: 0.3008 - val_loss: 0.1147 - val_mae: 0.1987\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1737 - mae: 0.2981 - val_loss: 0.1144 - val_mae: 0.1964\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1704 - mae: 0.2939 - val_loss: 0.1141 - val_mae: 0.1952\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.1699 - mae: 0.2940 - val_loss: 0.1304 - val_mae: 0.2546\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.1678 - mae: 0.2924 - val_loss: 0.1138 - val_mae: 0.1939\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 0.1670 - mae: 0.2914 - val_loss: 0.1272 - val_mae: 0.2453\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.1667 - mae: 0.2912 - val_loss: 0.1297 - val_mae: 0.2544\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.1655 - mae: 0.2892 - val_loss: 0.1166 - val_mae: 0.2087\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s 73us/step - loss: 0.1650 - mae: 0.2881 - val_loss: 0.1183 - val_mae: 0.2178\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s 76us/step - loss: 0.1613 - mae: 0.2842 - val_loss: 0.1219 - val_mae: 0.2301\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 0.1614 - mae: 0.2840 - val_loss: 0.1134 - val_mae: 0.1909\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.1597 - mae: 0.2822 - val_loss: 0.1175 - val_mae: 0.2128\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.1602 - mae: 0.2828 - val_loss: 0.1138 - val_mae: 0.1950\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.1593 - mae: 0.2816 - val_loss: 0.1130 - val_mae: 0.1871\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.1592 - mae: 0.2806 - val_loss: 0.1136 - val_mae: 0.1915\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.1578 - mae: 0.2790 - val_loss: 0.1138 - val_mae: 0.1950\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.1547 - mae: 0.2756 - val_loss: 0.1125 - val_mae: 0.1854\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.1553 - mae: 0.2764 - val_loss: 0.1126 - val_mae: 0.1856\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.1539 - mae: 0.2740 - val_loss: 0.1138 - val_mae: 0.1970\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.1532 - mae: 0.2735 - val_loss: 0.1167 - val_mae: 0.2125\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.1506 - mae: 0.2700 - val_loss: 0.1271 - val_mae: 0.2503\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.1525 - mae: 0.2730 - val_loss: 0.1124 - val_mae: 0.1847\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.1495 - mae: 0.2688 - val_loss: 0.1161 - val_mae: 0.2087\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s 77us/step - loss: 0.1500 - mae: 0.2682 - val_loss: 0.1170 - val_mae: 0.2161\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.1473 - mae: 0.2653 - val_loss: 0.1133 - val_mae: 0.1934\n",
      "Test loss: 0.11333238735049964\n",
      "Test accuracy: 0.19344626367092133\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200  # 訓練データを128ずつのデータに分けて学習させる\n",
    "epochs = 50 # 訓練データを繰り返し学習させる数\n",
    "\n",
    "# モデルの作成\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(100,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse',\n",
    " optimizer=RMSprop(),\n",
    " metrics=['mae'])\n",
    "\n",
    "# 学習は、scrkit-learnと同様fitで記述できる\n",
    "history = model.fit(x_train, tx_train,\n",
    " batch_size=batch_size,\n",
    " epochs=epochs,\n",
    " verbose=1,\n",
    " validation_data=(x_test, tx_test))\n",
    "\n",
    "# 評価はevaluateで行う\n",
    "score = model.evaluate(x_test, tx_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelの保存\n",
    "save_model_path = \"/Users/nagaiyuma/Desktop/maindata/model_3.h5\"\n",
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5006622524619104 0.28810336643133316\n"
     ]
    }
   ],
   "source": [
    "abs_error = abs(np.array(model.predict(x_test).reshape(1000,3)).T[1]- tx_test.T[1])\n",
    "ave_abs = np.average(abs_error)\n",
    "std_abs = np.std(abs_error)\n",
    "print(ave_abs, std_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
